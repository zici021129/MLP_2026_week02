{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eldA-sl8YsLp"
   },
   "source": [
    "# Week 2: Principal Component Analysis\n",
    "\n",
    "**Student Name 1, Student Name 2** \n",
    "\n",
    "In this workshop, we will work through a set of problems on dimensionality reduction -- a cannonical form of unsupervised learning. Within the machine learning pipeline, dimensionality reduction is an important tool, which can used in EDA to understand patterns in the data, feature engineering to create a low-dimensional representation of the inputs, and/or in the final phase when you are presenting and visualizing your solution.\n",
    "\n",
    "As usual, the worksheets will be completed in teams of 2-3, using **pair programming**, and we have provided cues to switch roles between driver and navigator. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    ">- Look for the üèÅ as cue to switch roles between driver and navigator.\n",
    ">- In some Exercises, you will see some beneficial hints at the bottom of questions.\n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given. \n",
    "\n",
    "As you work through the problems it will help to refer to your lecture notes (navigator). The exercises here are designed to reinforce the topics covered this week. Please discuss with the tutors if you get stuck, even early on! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. [Problem Definition and Setup](#setup)\n",
    "\n",
    "2. [Principal Component Analysis](#pca)\n",
    "\n",
    "    a. [Examining the Basis Vectors and Scores](#basis)\n",
    "\n",
    "    b. [Selecting the Number of Components](#nocomponents)\n",
    "\n",
    "    c. [Other Digits](#other)\n",
    "\n",
    "3. [Kernel PCA](#kpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm6XqPScK-FV"
   },
   "source": [
    "# Problem Definition and Setup <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "First, lets load in some packages to get us started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1674743647585,
     "user": {
      "displayName": "Jacob Page",
      "userId": "04818256562649201928"
     },
     "user_tz": 0
    },
    "id": "usEZAe6SYpV8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSj3Z820mFwv"
   },
   "source": [
    "## Data\n",
    "\n",
    "Our dataset will be the famous [MNIST](http://yann.lecun.com/exdb/mnist/) dataset of handwritten digits, which we will download from sklearn. The dataset consists of a set of greyscale images of the numbers 0-9 and corresponding labels. Usually the goal is to train a classifier (i.e. given an image, what digit does it correspond to?). Here we will throw away the labels and focus on the images themselves. Specifically, we will use dimensionality reduction to explore the images and underlying patterns and find a low-dimensional representation.\n",
    "\n",
    "First, load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784',parser = 'auto')\n",
    "X = mnist.data\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeHcm_OBGhtx"
   },
   "source": [
    "### üö© Exercise 1 (CORE)\n",
    "\n",
    "What is stored in `X` and `y` in the command above? What is the shape/datatype etc if an array?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5q1xG24emmdu"
   },
   "source": [
    "Now, let's create a dictionary, with the digit classes (0-9) as keys, where the correponding values are the set of all images corresponding to that particular label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1674743872754,
     "user": {
      "displayName": "Jacob Page",
      "userId": "04818256562649201928"
     },
     "user_tz": 0
    },
    "id": "nCExrjAlmQOm"
   },
   "outputs": [],
   "source": [
    "digits_dict = {}\n",
    "X_= X.values\n",
    "count = 0\n",
    "\n",
    "for label in y:\n",
    "  if label in digits_dict:\n",
    "    digits_dict[label] += [X_[count]]\n",
    "  else:\n",
    "    digits_dict[label] = [X_[count]]\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's visualize some of the images. We will start by picking a label and plotting a few images from within the dictionary. Note that each image contains a total of 784 pixels (28 by 28) and we will need to `reshape` the image to plot with `imshow(...,cmap='gray_r')`. Try also changing the label to view different digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylabel = '1'\n",
    "n_images_per_label = 5\n",
    "\n",
    "fig = plt.figure(figsize=(4*n_images_per_label, 4))\n",
    "for j in range(n_images_per_label):\n",
    "    ax_number = 1 + j\n",
    "    ax = fig.add_subplot(1, n_images_per_label, ax_number)\n",
    "    ax.imshow(digits_dict[mylabel][j].reshape((28,28)), cmap='gray_r')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z95pdmrynwyX"
   },
   "source": [
    "### üö© Exercise 2 (EXTRA)\n",
    "\n",
    "Edit the code above to plot a few images for multiple labels.\n",
    "\n",
    "<br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "\n",
    "Create a vector of labels and add additional for loop in the code above.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 873
    },
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1674743876957,
     "user": {
      "displayName": "Jacob Page",
      "userId": "04818256562649201928"
     },
     "user_tz": 0
    },
    "id": "98lyOLGKmjam",
    "outputId": "76cd9822-4e86-44d7-8bfe-7dfed42686fe"
   },
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R_1TGs8rInz"
   },
   "source": [
    "### üö© Exercise 3 (CORE)\n",
    "\n",
    "Now focus on the 3s only and create a data matrix called `X_threes`. Define also `N` (# datapoints) and `D` (# features).\n",
    "\n",
    "What are the features in this problem? How many features and data points are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1674743879981,
     "user": {
      "displayName": "Jacob Page",
      "userId": "04818256562649201928"
     },
     "user_tz": 0
    },
    "id": "HhzuQdIuptAh",
    "outputId": "274c220d-7d32-4942-a82c-49909efceef0"
   },
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAtBj2cRs6RZ"
   },
   "source": [
    "### üö© Exercise 4 (CORE)\n",
    "\n",
    "Now compute and plot the mean image of three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1674743895536,
     "user": {
      "displayName": "Jacob Page",
      "userId": "04818256562649201928"
     },
     "user_tz": 0
    },
    "id": "z3sD-Qyjs5vz",
    "outputId": "a32a5957-f43a-4e9c-8d34-7750f2e7e85b"
   },
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to first create a new data matrix that centers the data by subtracting the mean image, and then visualise some of the images and compare to the original data. Note: you will need to replace `X_three_mean` with the name you gave the mean image in the computation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1674743904757,
     "user": {
      "displayName": "Jacob Page",
      "userId": "04818256562649201928"
     },
     "user_tz": 0
    },
    "id": "Y3YJUQDIr_-G",
    "outputId": "d3cf35a0-db7d-4c25-b798-dfd013c16d49"
   },
   "outputs": [],
   "source": [
    "X_three_centred = X_threes - X_three_mean\n",
    "\n",
    "n_images = 5\n",
    "\n",
    "fig = plt.figure(figsize=(4*n_images, 4*2))\n",
    "for j in range(n_images):\n",
    "  ax = fig.add_subplot(2, n_images, j+1)\n",
    "  ax.imshow(X_three_centred[j,:].reshape((28,28)), cmap='gray_r')\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "\n",
    "  ax = fig.add_subplot(2, n_images, j+1+n_images)\n",
    "  ax.imshow(X_threes[j,:].reshape((28,28)), cmap='gray_r')\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 5 (CORE)\n",
    "\n",
    "Comment on whether or not the images need to be standardized before using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA <a id='pca'></a>\n",
    "\n",
    "Now, we will perform PCA to summarize the main patterns in the images. We will use the [`PCA()`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) transformer from the `sklearn.decomposition` package:\n",
    "\n",
    "- As we saw last week, we start by creating our transformer object, specifying any parameters as desired. For example, we can specify the number of components with the option `n_components`. If omitted, all components are kept.\n",
    "\n",
    "- Note that by default the `PCA()` transform centers the variables to have zero mean (but does not scale them). \n",
    "\n",
    "- After calling `.fit()`, our fitted object has a number of attributes, including:\n",
    "    - the mean accessible through the attribute `mean_`.\n",
    "    - the basis vectors (principal components) accesible through the `components_` attribute.\n",
    "\n",
    "- There are also a number of methods for the fitted object, including `.transform()` to obtain the low-dimensional representation (or also `fit_transform` combining both together). \n",
    "\n",
    " First, let's create the PCA transformer object and call `.fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_threes = PCA(n_components = 200)\n",
    "pca_threes.fit(X_threes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Basis Vectors and Scores <a id='basis'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 6 (EXTRA)\n",
    " \n",
    "Plot the mean image by accessing the `mean_` attribute and check that it is the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 7 (CORE)\n",
    "\n",
    "Plot the the first four basis vectors as images by accessing the `components_` attribute. What patterns do they seem describe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 8 (CORE)\n",
    "\n",
    "a) Use the `transform()` method to compute the PCA scores and save them in an object called `scores`. Then, plot the data points in the low-dimensional space spanned by the first two principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better interpret the latent dimensions, let's look at some projected points along each dimension and the corresponding images. Specifically, run the following code to:\n",
    "\n",
    "- first compute the $5, 25, 50, 75, 95\\%$ quantiles of the scores for the first two dimensions\n",
    "- then find the data point whose projection is closest to each combination of quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1q = np.quantile(scores[:,0],[.05,.25,.5,.75,.95])\n",
    "s2q = np.quantile(scores[:,1],[.05,.25,.5,.75,.95])\n",
    "\n",
    "idx = np.zeros([len(s1q),len(s2q)])\n",
    "\n",
    "for i in range(len(s1q)):\n",
    "    for j in range(len(s2q)):\n",
    "        aux = ((scores[:,0] - s1q[i])**2 + (scores[:,1] - s2q[j])**2).reshape(N,1)\n",
    "        idx[i,j] = np.where(aux == min(aux))[0][0]\n",
    "\n",
    "idx = idx.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Now, add these points in red to your plot above in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Run the following code to plot the images corresponding to this grid of points. Describe the general pattern of the first (left to right) and second (down to up) principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(s1q),len(s2q),figsize=(6,6))\n",
    "for i in range(len(s1q)):\n",
    "    for j in range(len(s2q)):\n",
    "        ax[len(s2q)-1-j,i].imshow(X_threes[idx[i,j],:].reshape((28,28)), cmap='gray_r')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try to create some artificial images, by fixing different values of the weights. This can also help to interpret the latent dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = np.quantile(scores[:,0],[.05,.25,.5,.75,.95])\n",
    "weight2 = 0\n",
    "\n",
    "images_pc1 = np.zeros([len(weight1),D])\n",
    "\n",
    "count = 0\n",
    "for w in weight1:   \n",
    "    images_pc1[count,:] =(pca_threes.mean_ + pca_threes.components_[0,:]*w+pca_threes.components_[1,:]*weight2)\n",
    "    count += 1\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(1,len(weight1),figsize=(10,6))\n",
    "for i in range(len(weight1)):\n",
    "    ax[i].imshow(images_pc1[i,:].reshape((28,28)), cmap='gray_r')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 9 (CORE)\n",
    "\n",
    "Repeat this to describe the third principal component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 10 (EXTRA)\n",
    "\n",
    "In lecture, we saw that we can also compute the basis vectors from an SVD decomposition of the data matrix. Use the `svd` function in `scipy.linalg` to compute the first three basis vectors and verify that they are the same (up to a change in sign -- note that the signs may be flipped because each principal component specifies a direction in the $D$-dimensional space and flipping the sign has no effect as the direction does not change). \n",
    "\n",
    "Does `PCA()` perform principal component analysis using an eigendecomposition of the empirical covariance matrix or using a SVD decomposition of the data matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!\n",
    "from scipy.linalg import svd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Number of Components <a id='nocomponents'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 11 (CORE)\n",
    "\n",
    "Next, let's investigate how many components are needed by considering how much variance is explained by each component.\n",
    "\n",
    "Note that the `pca_threes` object has an attribute `explained_variance_` (variance of each component) and `explained_variance_ratio_` (proportion of variance explained by each component). \n",
    "\n",
    "Plot both the proportion of variance explained and the cummulative proportion of variance explained. Provide a suggestion of how many components to use. How much variance is explained by the suggest number of components? Comment on why we may be able to use this number of components in relation to the total number of features.\n",
    "\n",
    "<br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "\n",
    "You can use `cumsum()` to compute the cummulative sum of the elements in a vector.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 12 (CORE)\n",
    "\n",
    "For your selected number of components, compute the reconstruted images. Plot the reconstruction for a few images and compare with the original images. Comment on the results.  \n",
    "\n",
    "<br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "\n",
    "You can use `inverse_transform()` to decode the scores.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Digits <a id='other'></a>\n",
    "\n",
    "Now, let's consider another digit. \n",
    "\n",
    "### üö© Exercise 13 (CORE)\n",
    "\n",
    "Perform PCA for another choice of digit. What do the first two components describe? Do some digits have better approximations than others? Comment on why this may be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cpde for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4-MKAQNiloz"
   },
   "source": [
    "### Exercise 14 (EXTRA)\n",
    "\n",
    "Finally, consider now two digits of your choice (edit the code below if you wish to pick different digits).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data \n",
    "X_twodigits = np.concatenate((digits_dict['3'], digits_dict['8']))\n",
    "N, D = X_twodigits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to compute and plot the mean and some of the principle components for this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfvsQJkHiMnq"
   },
   "outputs": [],
   "source": [
    "# Fit PCA\n",
    "pca_digits = PCA(n_components = 50)\n",
    "pca_digits.fit(X_twodigits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean image\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(pca_digits.mean_.reshape(28, 28), cmap='gray_r')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot basis vectors\n",
    "n_plot = 5\n",
    "fig, ax = plt.subplots(1,5,figsize=(10,4))\n",
    "for n in range(n_plot):\n",
    "  ax[n].imshow(pca_digits.components_[n,:].reshape((28,28)), cmap='gray_r')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the projection of the data in the latent space and color the data by the labels. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try also to generate artificial images and decsribe how images change along the PCs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA <a id='kpca'></a>\n",
    "\n",
    "Now, let's try using kernel PCA, which is available through sklearn's [`KernelPCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html) transformer. As usual we start by creating our object and specifying parameters (see documentation to learn more about the optional parameters). Then, we use the methods `.fit()` and `.transform()` to fit the object and obtain the lower-dimensional representation.\n",
    "\n",
    "In the code below, we use the radial basis function kernel, with the inverse bandwith parameter `gamma` set to 0.05. Setting, the option `fit_inverse_transform=True` will allow us to reconstruct the images later (and `alpha` is regularization used when inversing the transforming).\n",
    "\n",
    "_Note:_ we first subsampled the data, as kernel PCA can be slow on large datasets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Prepare data\n",
    "y_twodigits = y_twodigits.astype(int)\n",
    "X_twodigits = MinMaxScaler().fit_transform(X_twodigits)\n",
    "\n",
    "# Subsample the images (for speed)\n",
    "X_twodigits_subsampled, X_twodigits_test, y_twodigits_subsampled, y_twodigits_test = train_test_split(\n",
    "    X_twodigits, y_twodigits, stratify=y_twodigits, random_state=0, train_size=500, test_size=100\n",
    ")\n",
    "\n",
    "# Define our KPCA and PCA transformers\n",
    "n_components = 10\n",
    "kpca = KernelPCA(\n",
    "    n_components=n_components, kernel=\"rbf\", gamma=0.05, fit_inverse_transform=True, random_state=0, alpha=0.01)\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit and transform the data\n",
    "scores_kpca = kpca.fit_transform(X_twodigits_subsampled)\n",
    "scores_pca = pca.fit_transform(X_twodigits_subsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's plot the images in the space of the first two components for both kernel PCA and standard PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the images in the space of the first two components, colored by digit\n",
    "i, j = 0, 1 #component indicies\n",
    "yu =  [3,8] \n",
    "fig, ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "for dig in yu:\n",
    "    ax[0].scatter(scores_kpca[y_twodigits_subsampled==dig,i], \n",
    "               scores_kpca[y_twodigits_subsampled==dig,j],\n",
    "               c = colors[dig],label=dig)\n",
    "    ax[1].scatter(scores_pca[y_twodigits_subsampled==dig,i], \n",
    "               scores_pca[y_twodigits_subsampled==dig,j],\n",
    "               c = colors[dig],label=dig)\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('PCA%d' % (i+1))\n",
    "ax[0].set_ylabel('PCA%d' % (j+1))\n",
    "ax[0].set_title('Kernel PCA')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('PCA%d' % (i+1))\n",
    "ax[1].set_ylabel('PCA%d' % (j+1))\n",
    "ax[1].set_title('Standard PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Denoising\n",
    "Let's add some noise to our test images that weren't used in the fitting. We will then encode the noisy images into the latent space and then reconstruct our images, to see how well both methods are able to denoise the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the test images\n",
    "np.random.seed(0)\n",
    "noise = np.random.normal(0, 0.1, X_twodigits_test.shape)\n",
    "X_twodigits_test_noisy = X_twodigits_test + noise\n",
    "\n",
    "# Plot some noisy test images\n",
    "n_images = 5\n",
    "fig, ax = plt.subplots(2,n_images,figsize=(2*n_images, 4))\n",
    "for j in range(n_images):\n",
    "    ax[0,j].imshow(X_twodigits_test[j].reshape((28,28)), cmap='gray_r')\n",
    "    ax[1,j].imshow(X_twodigits_test_noisy[j].reshape((28,28)), cmap='gray_r')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "# Add titles\n",
    "ax[0,2].set_title('Original Images')\n",
    "ax[1,2].set_title('Noisy Images')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now transform the noisy test images using both PCA and KernelPCA\n",
    "scores_kpca_test = kpca.transform(X_twodigits_test_noisy)\n",
    "scores_pca_test = pca.transform(X_twodigits_test_noisy)\n",
    "\n",
    "# And reconstruct the noisy test images using both PCA and KernelPCA\n",
    "X_reconstructed_kpca = kpca.inverse_transform(\n",
    "    scores_kpca_test)\n",
    "X_reconstructed_pca = pca.inverse_transform(\n",
    "    scores_pca_test)\n",
    "\n",
    "# Plot some reconstructed images\n",
    "n_images = 5\n",
    "fig, ax = plt.subplots(4,n_images,figsize=(2*n_images, 8))\n",
    "for j in range(n_images):\n",
    "    ax[0,j].imshow(X_twodigits_test[j].reshape((28,28)), cmap='gray_r')\n",
    "    ax[1,j].imshow(X_twodigits_test_noisy[j].reshape((28,28)), cmap='gray_r')\n",
    "    ax[2,j].imshow(X_reconstructed_kpca[j].reshape((28,28)), cmap='gray_r')\n",
    "    ax[3,j].imshow(X_reconstructed_pca[j].reshape((28,28)), cmap='gray_r')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "# Add titles\n",
    "ax[0,2].set_title('Original Images')\n",
    "ax[1,2].set_title('Noisy Images')\n",
    "ax[2,2].set_title('Reconstructed Images (Kernel PCA)')\n",
    "ax[3,2].set_title('Reconstructed Images (Standard PCA)')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 15 (EXTRA)\n",
    "\n",
    "a) Try changing the `gamma`. What happens when you increase, e.g. `gamma=0.1`? Or decrease `gamma=0.01`? \n",
    "\n",
    "b) Try changing the number of components. How does this affect the reconstructed images for both PCA and kernel PCA?\n",
    "\n",
    "c) Which method would you prefer for this dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please **change 'Student 1' and 'Student 2' at the top of the notebook to include your name(s)**. \n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week02_key.ipynb "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "title": "MLP Workshop 2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
